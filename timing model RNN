#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SpO2 RNN (LSTM) trainer - FULL SCRIPT
- Reads 5 CSVs
- Target y = mean(SPO21, SPO22)
- Prefers user-specified feature list, falls back to all numeric columns if missing
- Time split 70/15/15
- Lookback window sequences
- Trains PyTorch LSTM, ALWAYS saves a checkpoint
- Loads checkpoint and evaluates on test set
"""

import os
import re
import json
import argparse
import numpy as np
import pandas as pd

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

# --------------- Utils ---------------

def set_seed(seed: int = 42):
    try:
        import random
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    except Exception:
        pass

def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize column names to lower-case and unify spaces/symbols."""
    def norm(c):
        c = str(c).strip()
        c = c.replace('\u2009', ' ').replace('\u00A0', ' ')
        c = c.replace('’', "'").replace('“','"').replace('”','"')
        c = c.lower()
        c = re.sub(r'[\s_/]+', ' ', c)
        c = c.replace('%', ' pct')
        c = c.replace('²', '2')
        c = c.replace('o₂', 'o2').replace('spo₂', 'spo2').replace('sao₂','sao2')
        c = re.sub(r'\s+', ' ', c)
        return c.strip()
    out = df.copy()
    out.columns = [norm(c) for c in out.columns]
    return out

def load_concat(paths):
    """Read multiple CSVs, tolerate ; or \\t separators, align columns, concat."""
    dfs = []
    for p in paths:
        if not os.path.exists(p):
            print(f"[WARN] File not found: {p}")
            continue
        try:
            df = pd.read_csv(p)
        except Exception:
            try:
                df = pd.read_csv(p, sep=';')
            except Exception:
                df = pd.read_csv(p, sep='\t')
        dfs.append(standardize_columns(df))
        print(f"[INFO] Loaded {p} with shape {df.shape}")
    if not dfs:
        raise SystemExit("No CSVs found among given paths.")
    all_cols = sorted(set().union(*[set(d.columns) for d in dfs]))
    dfs = [d.reindex(columns=all_cols) for d in dfs]
    big = pd.concat(dfs, axis=0, ignore_index=True)
    print(f"[INFO] Concatenated shape: {big.shape}")
    return big

def build_sequences(X: np.ndarray, y: np.ndarray, lookback: int):
    xs, ys = [], []
    for i in range(lookback, len(X)):
        xs.append(X[i-lookback:i, :])
        ys.append(y[i])
    if len(xs) == 0:
        return np.empty((0, lookback, X.shape[1])), np.empty((0,))
    return np.array(xs), np.array(ys)

# --------------- Dataset & Model ---------------

class SeqDS(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1,1)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

class LSTMReg(nn.Module):
    def __init__(self, in_dim: int):
        super().__init__()
        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=64, batch_first=True)
        self.drop = nn.Dropout(0.2)
        self.fc1  = nn.Linear(64, 32)
        self.relu = nn.ReLU()
        self.out  = nn.Linear(32, 1)
    def forward(self, x):
        o, _ = self.lstm(x)      # [B, T, 64]
        o   = o[:, -1, :]        # last timestep
        o   = self.drop(o)
        o   = self.relu(self.fc1(o))
        o   = self.out(o)        # [B, 1]
        return o

# --------------- Main ---------------

def main(a):
    set_seed(42)

    # 1) Load data
    paths = [a.f1, a.f2, a.f3, a.f4, a.f5]
    df = load_concat(paths)

    # 2) Build target y = mean(SPO21, SPO22) (case-insensitive)
    # After standardization, columns are lowercase, so look for 'spo21'/'spo22'
    target_names = [c for c in df.columns if c in ('spo21', 'spo22')]
    if not target_names:
        raise SystemExit("未找到目标列 SPO21 / SPO22（不区分大小写）。请确认 CSV 的列名。")
    target_df = df[target_names].apply(pd.to_numeric, errors='coerce')
    y_series = target_df.mean(axis=1).ffill().bfill()

    # 3) Features: prefer user's list; fallback to all numeric (except targets)
    preferred_features = [
        'pulse 1','pulse 2','pulse 3','pulse 4','pulse 5',
        'pi 1','pi 2','pi 3','pi 4','pi 5',
        'red drv 1','ir drv 1','dc gain 1','ac gain 1',
        'red drv 2','ir drv 2','dc gain 2','ac gain 2',
        'red drv 3','ir drv 3','dc gain 3','ac gain 3',
        'red drv 4','ir drv 4','dc gain 4','ac gain 4',
        'red drv 5','ir drv 5','dc gain 5','ac gain 5',
        'ecg 3','fio2 3','etco2 pct 3','etco2 mmhg 3','rr co2 3','rr ecg 3',
        'insp tv 3','exp tv 3','rig fio2'
    ]
    available_pref = [c for c in preferred_features if c in df.columns]
    if len(available_pref) < 5:
        print("[WARN] Preferred features too few; falling back to ALL numeric columns.")
        numeric_all = df.apply(pd.to_numeric, errors='coerce')
        feat_cols = [c for c in numeric_all.columns if c not in target_names]
    else:
        feat_cols = available_pref

    # 4) Numeric coercion & imputation
    X_df = df[feat_cols].apply(pd.to_numeric, errors='coerce')
    X_df = X_df.ffill().bfill()
    y_series = y_series.ffill().bfill()

    # sanity check
    if X_df.isna().any().any() or np.isnan(y_series.values).any():
        print("[WARN] Residual NaN found; replacing with column medians.")
        for c in X_df.columns:
            col = X_df[c]
            if col.isna().any():
                X_df[c] = col.fillna(col.median())
        if y_series.isna().any():
            y_series = y_series.fillna(y_series.median())

    X_all = X_df.to_numpy(dtype=float)
    y_all = y_series.to_numpy(dtype=float)
    n = len(X_all)
    print(f"[INFO] Usable rows: {n}, features used: {len(feat_cols)}")

    # 5) Time split 70/15/15
    n_tr = int(0.7 * n)
    n_va = int(0.15 * n)
    a1, b1 = 0, n_tr
    a2, b2 = b1, b1 + n_va
    a3, b3 = b2, n
    Xtr_raw, Xva_raw, Xte_raw = X_all[a1:b1], X_all[a2:b2], X_all[a3:b3]
    ytr_raw, yva_raw, yte_raw = y_all[a1:b1], y_all[a2:b2], y_all[a3:b3]
    print(f"[INFO] Split sizes -> train: {len(Xtr_raw)}, val: {len(Xva_raw)}, test: {len(Xte_raw)}")

    # 6) Standardize by train stats
    mu = Xtr_raw.mean(axis=0, keepdims=True)
    sigma = Xtr_raw.std(axis=0, keepdims=True) + 1e-8
    Xtr = (Xtr_raw - mu) / sigma
    Xva = (Xva_raw - mu) / sigma
    Xte = (Xte_raw - mu) / sigma

    # 7) Build sequences
    lb = a.lookback
    Xtr_seq, ytr = build_sequences(Xtr, ytr_raw, lb)
    Xva_seq, yva = build_sequences(Xva, yva_raw, lb)
    Xte_seq, yte = build_sequences(Xte, yte_raw, lb)
    print(f"[INFO] Seq shapes -> Xtr:{Xtr_seq.shape}, Xva:{Xva_seq.shape}, Xte:{Xte_seq.shape}")

    # 8) Device & model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = LSTMReg(in_dim=Xtr_seq.shape[2]).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=a.lr)
    loss_fn = nn.L1Loss()  # MAE

    # 9) DataLoader
    dl_tr = DataLoader(SeqDS(Xtr_seq, ytr), batch_size=a.batch_size, shuffle=True)

    # 10) Ensure outdir & checkpoint path
    os.makedirs(a.outdir, exist_ok=True)
    ckpt_path = os.path.join(a.outdir, "lstm_best.pt")
    saved_any = False
    best_val = float('inf')
    wait = 0
    patience = 5

    # If val is empty, use train as val (to allow saving/early stop logic)
    use_train_as_val = (len(Xva_seq) == 0)
    if use_train_as_val:
        Xva_seq, yva = Xtr_seq, ytr
        print("[WARN] 验证集为空，临时使用训练集进行验证与早停。")

    # 11) Training loop (ALWAYS save at epoch 1; then on improvement)
    for epoch in range(a.epochs):
        model.train()
        running = 0.0
        for xb, yb in dl_tr:
            xb, yb = xb.to(device), yb.to(device)
            opt.zero_grad()
            pred = model(xb)
            loss = loss_fn(pred, yb)
            if not torch.isfinite(loss):
                raise RuntimeError("Loss is NaN/Inf. Check data preprocessing.")
            loss.backward()
            opt.step()
            running += loss.item() * len(xb)
        train_loss = running / max(1, len(Xtr_seq))

        # Validation
        model.eval()
        with torch.no_grad():
            xv = torch.tensor(Xva_seq, dtype=torch.float32).to(device)
            yv = torch.tensor(yva, dtype=torch.float32).view(-1,1).to(device)
            val_pred = model(xv)
            val_loss = loss_fn(val_pred, yv).item()

        print(f"[EPOCH {epoch+1}/{a.epochs}] train_MAE={train_loss:.6f}  val_MAE={val_loss:.6f}")

        # Save policy: first epoch ALWAYS save; later save if improves
        if (epoch == 0) or (val_loss < best_val - 1e-6):
            best_val = val_loss
            wait = 0
            torch.save(model.state_dict(), ckpt_path)
            saved_any = True
            # Save scalers & meta alongside
            np.save(os.path.join(a.outdir, "mu.npy"), mu)
            np.save(os.path.join(a.outdir, "sigma.npy"), sigma)
            with open(os.path.join(a.outdir, "meta.json"), "w", encoding="utf-8") as f:
                json.dump({"features": feat_cols, "lookback": lb, "targets": target_names}, f, indent=2, ensure_ascii=False)
            print(f"[INFO] Saved checkpoint -> {ckpt_path}")
        else:
            wait += 1
            if wait >= patience:
                print(f"[INFO] Early stopping at epoch {epoch+1}. Best val: {best_val:.6f}")
                break

    # Final fallback save (in case nothing was saved)
    if not saved_any:
        torch.save(model.state_dict(), ckpt_path)
        np.save(os.path.join(a.outdir, "mu.npy"), mu)
        np.save(os.path.join(a.outdir, "sigma.npy"), sigma)
        with open(os.path.join(a.outdir, "meta.json"), "w", encoding="utf-8") as f:
            json.dump({"features": feat_cols, "lookback": lb, "targets": target_names}, f, indent=2, ensure_ascii=False)
        print("[WARN] 训练期间未触发保存，已保存最后一轮模型到 lstm_best.pt。")

    # 12) Load checkpoint for evaluation (if missing, use in-memory model)
    try:
        model.load_state_dict(torch.load(ckpt_path, map_location=device))
        print(f"[INFO] Loaded checkpoint for evaluation -> {ckpt_path}")
    except FileNotFoundError:
        print(f"[WARN] 未找到 {ckpt_path}，改用当前内存模型评估。")

    # 13) Evaluate on test
    model.eval()
    with torch.no_grad():
        Xte_t = torch.tensor(Xte_seq, dtype=torch.float32).to(device)
        y_pred = model(Xte_t).cpu().numpy().reshape(-1)

    y_true = yte.reshape(-1)
    if len(y_true) == 0:
        mae = rmse = r2 = float("nan")
        print("[WARN] 测试集为空，指标以 NaN 记。")
    else:
        mae = float(np.mean(np.abs(y_pred - y_true)))
        rmse = float(np.sqrt(np.mean((y_pred - y_true) ** 2)))
        ss_res = float(np.sum((y_true - y_pred) ** 2))
        ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2)) + 1e-8
        r2 = float(1.0 - ss_res / ss_tot)

    result = {"MAE": mae, "RMSE": rmse, "R2": r2, "saved_to": a.outdir}
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    p = argparse.ArgumentParser()
    # Default file names; change if yours不同
    p.add_argument("--f1", default="100001.csv")
    p.add_argument("--f2", default="100002.csv")
    p.add_argument("--f3", default="100003.csv")
    p.add_argument("--f4", default="100004.csv")
    p.add_argument("--f5", default="100005.csv")
    p.add_argument("--lookback", type=int, default=60, help="sequence length")
    p.add_argument("--epochs", type=int, default=30)
    p.add_argument("--batch_size", type=int, default=256)
    p.add_argument("--lr", type=float, default=1e-3)
    p.add_argument("--outdir", default="spo2_rnn_torch_out")
    args = p.parse_args()
    main(args)
